---
title: 'Spatial Data Manipulation, Analysis and Visualization in R: The Basics'
author: "Marc Weber"
date: "Wednesday, February 18 2015"
output:
  ioslides_presentation:
    fig_height: 9
    fig_width: 9
    keep_md: yes
width: 1656
height: 1035
logo: epa_seal.png
biglogo: epa_seal.png
---


## The Roadmap {.smallest .columns-2}
<div class="notes">
- I've been using R for about a decade, started doing spatial work in R about 5 years ago

- Impetus for doing spatial work in R was documenting workflow in same language most folks I work with use - R

- Now do about 50% or more of spatial work in R, but even when I don't use R, I really do use R - will explain later

- Talk will be a methods talk, not a demonstration of project, will go over a lot of foundation stuff many people may already know or have heard, but will hopefully have some interesting stuff later in talk even for those used to doing spatial work in R
</div>
- Structure of spatial data in R
- Reading / Writing Spatial Data
- Visualizing Spatial Data
- Analysing Spatial Data

<img src="roadmap.jpg" alt="Roadmap" style="width: 300px;"/>

## Explosion of Spatial Packages in R Recently
<div class="centered">
```{r fig.width=7, fig.height=4, echo=FALSE}
library(png)
library(grid)
img <- readPNG('SpatialPackages.png')
 grid.raster(img)
```
</div>

- Spatial package dependencies on sp package in R (July 2011) from Roger Bivand talk
http://geostat-course.org/system/files/monday_slides.pdf 

## Get to know sp
<div class="notes">
- sp package is foundation for spatial objects in R, used by most spatial packages

- Key point is that sp uses S4 new style classes 

- S4 classes have formal definitions for components, or slots, that classes contain 

- These formal constraints are useful for spatial objects (i.e. bounding boxes have to have x and y mins an maxes, spatial things have coordinate reference systems, ets)
</div>
<div class="centered">
```{r fig.width=8, fig.height=5, echo=FALSE}
library(png)
library(grid)
img <- readPNG('SpatialObjects.png')
 grid.raster(img)
```
</div>

## sp slots mirror the structure of ESRI shapefiles
Mandatory Components                                       Optional Components
---------------------------------------------------------  -------------
.shp - actual geometry of feature                          .prj - CRS and projection info in WKT format
.shx - shape index - binary file giving position index     .sbn and .sbx- spatial indexing files
.dbf - attribute information, stored in dBase IV format    .xml - metadata file

## Get to know sp objects {.smaller .columns-2}
<div class="notes">
- Here we're loading in a spatial points data frame of norwegian peaks over 2000 meters that comes with the rgdal package (the r implementation of the geospatial data abstraction library), one of workhorse spatial packages in R

- Remember that sp uses new style classes - the class of an object in R determines the method used

- Therefore, we can use plot and summary on sp spatial objects and get appropriate results
</div>
```{r, warning=FALSE, message=FALSE, out.width= '450px', comment=NA}
library(rgdal)
data(nor2k)
class(nor2k)
slotNames(nor2k)
plot(nor2k, axes=T, 
     main='Peaks in Norway over 2000 meters')
```

## Get to know sp objects {.smaller}
<div class="notes">
- Summary method with sp akin to summary on data frame, but provides some useful overview information for spatial objects like the class, the bounding box, coordinate reference system, synopsis of attributes in data table
</div>
```{r, warning=FALSE, message=FALSE, comment=NA}
library(rgdal)
data(nor2k)
summary(nor2k)
```

## Get to know raster {.smallest .columns-2 .build}
<div class="notes">
- A big advantage of the raster package is that it can work with data on disk that's too large to load into memory for R

- raster package creates objects from files that only contain information about the structure of the data, like number of rows and columns, extent, but doesn't try to read all the cell values into memory

- Processes data in chunks when running computations on rasters
</div>
>- sp has spatial grid and pixel classes, but raster package is best for raster data in R
>- Raster data structure divides region into rectangles / cells 
>- Cells can store one or more values for the cells
```{r, message=FALSE,warning=FALSE, out.width= '450px'}
library(raster)
r <- raster(ncol=10, nrow=10)
values(r) <- c(1:100)
plot(r, main='Raster with 100 cells')
```

## Making data spatial {.smaller .build}
<div class="notes">
- Let's say we have a table in an R dataset or a csv that has coordinate information - we can 'promote' to a spatial object in sp
- In this case, we're just looking at a data frame of US cities in the maps package
</div>
```{r, results='asis', out.width= '250px', message=FALSE, warning=FALSE, comment=NA}
library(maps);library(sp);require(knitr)
data(us.cities)
knitr::kable(us.cities[1:5,])
class(us.cities) # simple data frame
```

## Making data spatial {.smaller .build}
### Promote a data frame with coordinate to an sp SpatialPointsDataFrame
```{r, results='asis', out.width= '200px', message=FALSE, warning=FALSE, comment=NA, fig.align='center'}
library(maps);library(sp)
data(us.cities)
coordinates(us.cities) <- c("long", "lat") 
class(us.cities) 
plot(us.cities, pch = 20, col = 'forestgreen', axes=T,
     xlim=c(-125,-70), ylim=c(26,55))
```

## Maps package provides convenient stock maps {.smaller .columns-2}
<div class="notes">
- It's handy to use existing administrative units in R or from online databases when mapping in R
</div>
```{r, message=FALSE, warning=FALSE, out.width= '275px'}
library(maps)
par(mfrow=c(1,1))
map()
map.text('county','oregon')
map.axes()
title(main="Oregon State")
```

Loading administrative backgrounds from Global Administrative Areas is another good option (http://gadm.org/)

## Dealing with coordinate reference systems in R {.smaller .columns-2}
- CRS can be geographic (lat/lon), projected, or NA in R
- Data with different CRS MUST be transformed to common CRS in R
- Projections in sp are provided in PROJ4 strings in the proj4string slot of an object
- http://www.spatialreference.org/

- Useful rgdal package functions:
    * projInfo(type='datum')
    * projInfo(type='ellps')
    * projInfo(type='proj')

<img src="CRS.png" alt="CRS" style="width: 550px;"/>


## Dealing with coordinate reference systems in R 
- For sp classes:
    - To get the CRS:  proj4string(x)
    - To assign the CRS:
        - Use either EPSG code or PROJ.4:
            - proj4string(x) <- CRS("init=epsg:4269")
            - proj4string(x) <- CRS("+proj=utm +zone=10 +datum=WGS84")
    - To transform CRS
          - x <- spTransform(x, CRS("+init=epsg:4238"))
          - x <- spTransform(x, proj4string(y))

- For rasters:
    - To get the CRS:  projection(x)
    - To transform CRS:  projectRaster(x)

## Reading and writing spatial data {.build}
- Best method for reading and writing shapefiles is to use readOGR() and writeOGR() in rgdal
```{r, eval=FALSE}
library(rgdal)
setwd('K:/GitProjects/RUserWebinar')
HUCs <- readOGR('.','OR_HUC08')
writeOGR(HUCs, '.','HUC', driver='ESRI Shapefile')
```
- Best method for reading and writing rasters is raster package
```{r, eval=FALSE}
library(raster)
r <- raster('clay.tif')
# crop it
r <- crop(r, extent(-1000000, 2000000, 100000, 3000000) )
writeRaster(r, 'clay_smaller.tif',format='GTiff')
```

## Understanding slot structure { .build}
<div class="notes">
- Knowing how information is stored in slots, we can pull out area in a couple different ways from a spatial polygons data frame 
- Here we'll start looking at some example analyses using a spatial polygons data frame of USGS Hydrologic Units in Oregon
- Using the structure function is a nice way to get to understand slots in S4 spatial objects
</div>
```{r, eval=TRUE, echo=TRUE, comment=NA, message=FALSE, warning=FALSE}
require(sp);require(rgeos);load("K:/GitProjects/RUserWebinar/Data.RData")
# A spatial PolygonsDataFrame has 5 slots:
str(HUCs, 2)
```

## Understanding slot structure { .build}
```{r, eval=TRUE, echo=TRUE, comment=NA, message=FALSE, warning=FALSE}
require(sp);require(rgeos);load("K:/GitProjects/RUserWebinar/Data.RData")
# Each polygon element has 5 of it's own slots - here we look at first one:
str(HUCs@polygons[[1]])[]
```

## Understanding slot structure {.smaller .build}
```{r, eval=TRUE, echo=TRUE, comment=NA, message=FALSE, warning=FALSE}
require(sp);require(rgeos); load("K:/GitProjects/RUserWebinar/Data.RData")
# Here we get a vector of area for features in HUCs spdf
area(HUCs[1:4,])
# Total Area - gArea function in rgeos gives same result
sum(area(HUCs))
# Area of a particular feature
HUCs@polygons[[1]]@Polygons[[1]]@area
```

## Getting areas of polygons {.smaller .build}
```{r, eval=TRUE, echo=TRUE, comment=NA, out.width= '400px', message=FALSE, warning=FALSE, fig.align='center'}
require(sp);load("K:/GitProjects/RUserWebinar/Data.RData")
plot(HUCs, axes=T, main='HUCs in Oregon')
```

## Getting areas of polygons {.smaller .columns-2 .build}
<div class="notes">
- Here we leverage the area slot of the polgons slot to write a simple function to generate a percent of total area for each HUC in our spatial polygons data frame
- First we use sum and sapply over area slots of polygon slots to derive our total area of all features
- Next, we again use sapply over our area slots of polygon slots but dividing by total area for each feature to get percent area
- Finally, we apply the function to our HUCs by indexing to just plot HUCs larger than a defined percent of area in a different color
</div>
- Highlight larger HUCs using Area function
```{r, eval=TRUE, echo=TRUE, comment=NA, out.width= '400px', message=FALSE, warning=FALSE}
require(sp)
load("K:/GitProjects/RUserWebinar/Data.RData")
plot(HUCs, axes=T, main='HUCs in Oregon')
# Function to calculate percent of area
AreaPercent <- function(x) {
  tot_area <- sum(sapply(slot(x, "polygons"),
                         slot, "area"))
  sapply(slot(x, "polygons"), slot, 
         "area") / tot_area * 100
}  
# just plot bigger HUCs
plot(HUCs[AreaPercent(HUCs) > 1,], add=T, 
     col='Steel Blue')
```

## Spatial Operations on vector data {.build .smaller .columns-2}
<div class="notes">
- Now we'lll step through some examples of spatial operations on vector data
- Here we're looking at a spatial points data frame of stream gages in Oregon
- First step in any spatial analysis should ALWAYS be setting everything to same CRS
- show code for how I found EPSG code for Oregon Lambert projection!
- Here we use ggplot from ggplot2 package to plot our gages and color the points by log of flow
</div>
```{r, echo=TRUE, message=FALSE, warning=FALSE, out.width= '450px'}
require(ggplot2)
load("K:/GitProjects/RUserWebinar/Data.RData")
# Take a look at some USGS stream gages for PNW
gages@data[1:5,5:8]
# Explicitly set CRS for layers
gages <- spTransform(gages, 
                     CRS('+init=epsg:2991'))
# Locations of gages
ggplot(gages@data, aes(LON_SITE, LAT_SITE)) + 
  geom_point(aes(color=log10(FLOW))) + 
  coord_equal()
```

## Spatial Operations on vector data {.build .smaller .columns-2}
### Spatial Indexing 
<div class="notes">
- Here we can see we subset our HUCs by those less than a certain longitude using an attribute index of HUCs whose longitude is less than 400,000
- Then we plot all our stream gages over that - our gages extend beyond the bounds of our subset HUCs
</div>
- Select gages within a set of HUCs
```{r, echo=TRUE, message=FALSE, warning=FALSE, out.width= '400px', fig.align='center'}
load("K:/GitProjects/RUserWebinar/Data.RData")
gages_proj <- spTransform(gages, 
                          CRS('+init=epsg:2991'))
HUCs_proj <- spTransform(HUCs, 
                         CRS('+init=epsg:2991'))
HUCs_proj$LON <- coordinates(HUCs_proj)[,1]
HUCs_west <- HUCs_proj[HUCs_proj$LON < 400000, ]
options(scipen=3)
plot(gages_proj, axes=T, col='blue')
plot(HUCs_west, add=T)
```

## Spatial Operations on vector data {.build .smaller .columns-2}
### Spatial Indexing 
<div class="notes">
- It's as simple as an index operation to subset the gages to just those within our new HUCs
</div>
- Select gages within a set of HUCs

```{r, echo=TRUE, message=FALSE, warning=FALSE, out.width= '400px', fig.align='center'}
load("K:/GitProjects/RUserWebinar/Data.RData")
gages_west <- gages_proj[HUCs_west,]
plot(HUCs_west, axes=T)
plot(gages_west, add=T, col='blue')
```

## Spatial Operations on vector data {.build .smaller}
### Overlay and Aggregation - What HUCs are gages located in?
<div class="notes">
- Another typical GIS question we might ask of spatial data is what HUCs are our gages located in?
- Here we roll together couple simple steps in a function to update our gages data slot with the ID of the HUC each gage is within
- The first line in our function uses the 'over' from sp to overlay our gages and HUCs - what it returns is a data frame containing the HUC attributes corresponding to each of our 622 stream gages
- We then have to take the additional step of rolling this result back into the data slot of our gages spatial points data frame
</div>
```{r, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
load("K:/GitProjects/RUserWebinar/Data.RData")
OverUpdate <- function(points, polys) {
  pointpoly <- over(points, polys)
  points@data <- data.frame(points@data, pointpoly)
}
gages_proj@data <- OverUpdate(gages_proj, HUCs_proj)
head(gages_proj@data[,c(3,12)])
```

## Spatial Operations on vector data {.smaller .build}
### Overlay and Aggregation 
<div class="notes">
- Here rather than identifying which HUC each gage goes with, we can summarize some attribue of gages for all the gages within each HUCs
- In our over function here, you can see we specify a field of our gages to use (flow), and then give a function parameter of mean to over as well
- Can of course use sum, median, or some other function on any attribute of spatial data
</div>
- What is mean streamflow based on gages within each HUC?
```{r, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
load("K:/GitProjects/RUserWebinar/Data.RData")
HUCs_proj$StreamFlow <- over(HUCs_proj,gages_proj[8],fn = mean)
head(HUCs_proj@data[!is.na(HUCs_proj@data$StreamFlow),])
```

## Attribute joining {.smaller.build}
<div class="notes">
- Often we want to combine spatial data with attributes from another table or a csv file
- Here we'll use a data frame of cities with population information
</div>
```{r, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
load("K:/GitProjects/RUserWebinar/Data.RData")
head(cities[,c(3:4, 7:8)])
```

## Attribute joining {.smaller.build}
<div class="notes">
- Next we'll join to our data slot of gages spatial points data frame in a way that does not scramble the ordering between data and spatial slots
- match or join are good options to use - I typically use match.  Be very careful about using merge - you'll need to make sure your data slot and spatial slot are ordered the same or you'll scramble results
</div>
```{r, echo=TRUE, message=FALSE, warning=FALSE, comment=NA}
require(plyr);load("K:/GitProjects/RUserWebinar/Data.RData")
# We can use match or join to connect to our spatial gages 
# gages$POP2007 <- cities$POP2007[match(gages$NearCity, cities$NAME)]
# OR set a common name field and use join from plyr
names(gages)[7] <- "NAME"
gages@data <- join(gages@data, cities)
head(gages@data[,c(3,18)])
```

## Spatial operations on vector data {.build .smaller .columns-2}
### Dissolving
<div class="notes">
-  Some of the common tasks we want to do in a GIS like dissolving, buffering, unioning, intersecting we can do in R using the rgeos package
- Here we use the cut function with quanitles to generate four 'longitude' bins
- We then apply those bins as a parameter in the rgeos gUnaryUnion function to dissolve our HUCs on four classes or longitude and generate four larger units out of the HUCs 
</div>
- create four bins of longitude values using coordinate data from HUCs
```{r, echo=TRUE, message=FALSE, warning=FALSE, out.width= '350px', fig.align='center'}
library(rgeos); library(rgdal)
lps <- coordinates(HUCs)
IDFourBins <- cut(lps[,1], quantile(lps[,1]), 
                  include.lowest=TRUE)
regions = gUnaryUnion(HUCs,IDFourBins)
regions = SpatialPolygonsDataFrame(regions,
          data.frame(regions = c('Coastal',
          'Mountains','High Desert','Eastern')),
          match.ID = FALSE)
plot(regions, axes=T)
text(coordinates(regions), 
     label = regions$regions, cex=.8)
```

## Working with rasters {.build .smaller .columns-2}
<div class="notes">
- We've been focusing on vector data, so let's loook a bit at raster data
- Here we'll read in a raster of USGS STATSGO clay for the US
- raster package handles spatial data differently 
- As I mentioned earlier, a big advantage of the raster package is that it can work with data on disk that's too large to load into memory for R, and here we use the helper inMemory fuction to verify that our raster is not in memory in R
- Here we use cellStats function in raster to get our overall min and max cell values, projection to get our rasters' projeciton info, and plot to plot our raster
</div>
```{r, echo=TRUE, message=FALSE, warning=FALSE, out.width= '300px', comment=NA}
require(raster)
load("K:/GitProjects/RUserWebinar/Data.RData")
clay <- raster('clay.tif')
inMemory(clay)
cellStats(clay, min); cellStats(clay, max)
projection(clay)
plot(clay)
```

## Working with rasters {.build .smaller}
<div class="notes">
-  We can crop a raster by passing it some extent coordinates as well as by using another raster or spatial object from which extent can be extracted 
</div>
```{r, echo=TRUE, message=FALSE, warning=FALSE, out.width= '300px', comment=NA, fig.align='center'}
require(raster)
clay <- raster('clay.tif')
clay_OR <- crop(clay, extent(-2261000, -1594944, 2348115, 2850963))
plot(clay_OR)
```

## Working with rasters {.build .smaller}
<div class="notes">
-  We can use the extract function in the raster package much like over in sp, to derive clay content at our gage stations
</div>
```{r, echo=TRUE, message=FALSE, warning=FALSE, out.width= '400px', comment=NA}
require(raster)
clay <- raster('clay.tif')
gages <- spTransform(gages, CRS(projection(clay)))
gages$clay = extract(clay,gages)
head(gages@data[,c(3,12)])
```

## Visualizing data {.build .smaller .columns-2}
### RasterVis
<div class="notes">
- Now we'll step through some visualization examples, starting with the rasterVis package since we just looked at raster data
- rasterVis builds off the raster package and provides methods for enhanced visualization and interaction with raster data in R
- See great examples at http://oscarperpinan.github.io/rastervis/
- Here we use the handy 'getData' function in raster package to grab altitude data at 2.5 are-minute resolution from the global climage data center and global administrative areas I mentioned on earlier slide
- We then subset our admin areas to the state of Oregon, crop our altitude raster to Oregon, and use levelplot function in rasterVis to make a nice levelplot visualization
</div>
```{r, echo=TRUE, warning=FALSE, message=FALSE,out.width= '500px'}
library(rasterVis)
alt <- getData('worldclim', var='alt', res=2.5)
usa1 <- getData('GADM', country='USA', level=1)
oregon <- usa1[usa1$NAME_1 == 'Oregon',]
alt <- crop(alt, extent(oregon) + 0.5)
alt <- mask(alt, oregon)
levelplot(alt, par.settings=GrTheme)
```

## Visualizing data
### PlotGoogleMaps
<div class="notes">
- This package was new to me until last week and was pointed out by Mike Papenfus here at our lab
- Allows you to map your points, lines and polygons straight onto Google Maps, and further, allows you to do interactive pop-ups of your spatial data overlaid on Google Maps
</div>
```{r, echo=TRUE, warning=FALSE, message=FALSE, eval=FALSE}
require(plotGoogleMaps)
HUCs <- spTransform(HUCs, CRS('+init=epsg:28992'))
map <- plotGoogleMaps(HUCs, filename='RGoogleMapsExample.htm')
```
<div class="centered">
<img src="PlotGoogleMapsExample.png" alt="Roadmap" style="width: 400px;"/>
</div>

## Visualizing data using ggmap
<div class="notes">
- Handy package is ggmap - basically allows you to visualize spatial data on top of Google Maps, OpenStreetMaps, Stamen  Maps or Cloud Made maps using ggplot2
- Unfortunately using stamen maps from ggmap is broken at the moment so can't use
- Fix involves editing function in package
</div>
```{r, echo=TRUE, warning=FALSE, message=FALSE,out.width= '400px', fig.align='center'}
library(ggmap)
mymap <- get_map(location = "Corvallis, OR", source="google", maptype="terrain",zoom = 12)
ggmap(mymap, extent="device")
```

## Visualizing data
### Taking it a step further with ggmap
<div class="notes">
- You can plot points, lines and polygons on ggmap for nice visualizations
- Here I just have an example of arranging several plots based on gpx coordinates from my watch from a weekend run using ggplot and ggmap
- Just an example of the kind of melding of data and maps you can do in R with ggplot2 and ggmap
- I don't show code here, but I have all the code posted up on my GitHub page
</div>
```{r, eval=FALSE, echo=FALSE}
# Modified slightly From R-blggers http://www.r-bloggers.com/stay-on-track-plotting-gps-tracks-with-r/

library(XML)
library(OpenStreetMap)
library(lubridate)
library(gridExtra)
# a function that shifts vectors conviniently
  
  shift.vec <- function (vec, shift) {
    if(length(vec) <= abs(shift)) {
      rep(NA ,length(vec))
    }else{
      if (shift >= 0) {
        c(rep(NA, shift), vec[1:(length(vec)-shift)]) }
      else {
        c(vec[(abs(shift)+1):length(vec)], rep(NA, abs(shift))) } } }

# Now, we're reading in the GPX file. If you want help on parsing XML files, check out this (German) tutorial I made a while ago.

# Parse the GPX file
pfile <- htmlTreeParse("activity_659032443.gpx",error = function (...) {}, useInternalNodes = T)
# Get all elevations, times and coordinates via the respective xpath
elevations <- as.numeric(xpathSApply(pfile, path = "//trkpt/ele", xmlValue))
times <- xpathSApply(pfile, path = "//trkpt/time", xmlValue)
coords <- xpathSApply(pfile, path = "//trkpt", xmlAttrs)
# Extract latitude and longitude from the coordinates
lats <- as.numeric(coords["lat",])
lons <- as.numeric(coords["lon",])
# Put everything in a dataframe and get rid of old variables
geodf <- data.frame(lat = lats, lon = lons, ele = elevations, time = times)
rm(list=c("elevations", "lats", "lons", "pfile", "times", "coords"))
head(geodf)

# We already have nice dataframe now with all the information available in the GPX file for each position. 
# Each position is defined by the latitude and longitude and we also have the elevation (altitude) available. 
# Note, that the altitude ist quite noisy with GPS, we will see this in a minute.

# Now, let's calculate the distances between successive positions and the respective speed in this segment.

# Shift vectors for lat and lon so that each row also contains the next position.
geodf$lat.p1 <- shift.vec(geodf$lat, -1)
geodf$lon.p1 <- shift.vec(geodf$lon, -1)
# Calculate distances (in metres) using the function pointDistance from the 'raster' package.
# Parameter 'lonlat' has to be TRUE!
geodf$dist.to.prev <- apply(geodf, 1, FUN = function (row) {
  pointDistance(c(as.numeric(row["lat.p1"]),
                  as.numeric(row["lon.p1"])),
                c(as.numeric(row["lat"]), as.numeric(row["lon"])),
                lonlat = T)
})
# Transform the column 'time' so that R knows how to interpret it.
geodf$time <- strptime(geodf$time, format = "%Y-%m-%dT%H:%M:%OS")
# Shift the time vector, too.
geodf$time.p1 <- shift.vec(geodf$time, -1)
# Shift the elevation vector, too.
geodf$ele.p1 <- shift.vec(geodf$ele, -10)
# Calculate the number of seconds between two positions.
geodf$time.diff.to.prev <- as.numeric(difftime(geodf$time.p1, geodf$time))
# Calculate elevation difference between two positions.
geodf$elev.diff.to.prev <- as.numeric(geodf$ele.p1 - geodf$ele)
# Calculate metres per seconds, kilometres per hour and two LOWESS smoothers to get rid of some noise.
geodf$speed.m.per.sec <- geodf$dist.to.prev / geodf$time.diff.to.prev
geodf$speed.km.per.h <- geodf$speed.m.per.sec * 3.6
geodf$speed.km.per.h <- ifelse(is.na(geodf$speed.km.per.h), 0, geodf$speed.km.per.h)
geodf$pace <- geodf$speed.km.per.h * 96.56064
geodf$lowess.speed <- lowess(geodf$speed.km.per.h, f = 0.2)$y
geodf$lowess.ele <- lowess(geodf$ele, f = 0.2)$y
geodf$pace <- 96.56064 /geodf$speed.km.per.h
geodf$slope <- geodf$elev.diff.to.prev / geodf$dist.to.prev
geodf$pace[geodf$pace==Inf] <- 0
geodf <- na.omit(geodf)
head(geodf)
# Plot the track without any map, the shape of the track is already visible.
basic_route <-ggplot(geodf, aes(x=lon, y=lat)) + geom_point() +
  xlab("Longitude") + ylab("Lattitude") + ggtitle("Basic Route of Run")

# Plot elevations and smoother
elevation <- ggplot(geodf, aes(x=time, y=ele)) +
  geom_point() + geom_line() + ylab("Elevation in Meters") +
  xlab("Time") + geom_smooth() + ggtitle('Elevation During Run')


# Plot pace
pace <- ggplot(geodf, aes(x=time, y=pace)) +
  geom_point() + geom_line() + ylab("Minute/Mile Pace") +
  xlab("Time") + geom_smooth() + ylim(6,10) + ggtitle('Pace During Run')


# We will now use the OpenStreetMap package to get some maps from the internet and use them as a background for the track we just converted. There are several blocks for each map type. Check the comments in the first block what the different calls do.

# First, we need to get the specific map. The 'type' argument controls which type of map we get.

map <- openmap(as.numeric(c(max(geodf$lat), min(geodf$lon))),
as.numeric(c(min(geodf$lat), max(geodf$lon))), type = "osm")

# This next step is important and it took me a while to figure this out. 
# We need to convert the maps we got with the function openmap() to a projection that fits our longitude-latitude format of positions. 
# This will distort the maps in the plots a little. But we need this step because the track has to fit the map!

transmap <- openproj(map, projection = "+proj=longlat")

# Now for plotting...
png("map1.png", width = 1000, height = 800, res = 100)
par(mar = rep(0,4))
plot(transmap, raster=T)
lines(geodf$lon, geodf$lat, type = "l", col = scales::alpha("red", .5), lwd = 4)
dev.off()

library(ggplot2)
data(seals)
head(seals)
head(geodf)
p <- ggplot(geodf, aes(x = lon, y = lat))
p + geom_segment(aes(xend = lon.p1, yend = lat.p1))


library(ggmap)
library(jpeg)
library(plyr) 
# pass bounding box with extent of gpx tracks
myLocation <- c(lon=-123.240, lat=44.663)
myLocation <- c(-123.26, 44.650, -123.220, 44.675)
myMap <- get_map(location=myLocation,maptype = "terrain",source = "stamen")

mp <- ggmap(myMap, extent="panel", maprange=FALSE) + 
  geom_segment(data=geodf, mapping=aes(x=lon, y=lat, xend=lon.p1, yend=lat.p1), size=2, color="blue")
mp  

grid.arrange(basic_route, elevation, pace, mp)
```
<div class="centered">
<img src="SundayRun.png" alt="Roadmap" style="width: 500px;"/>
</div>

## Exploratory Spatial Data Analysis with micromap
<div class="notes">
- I'd be remiss not to point out our micromap package that's up on CRAN, article on package in latest issue of JSS
- The micromap package provides a means to read in shapefiles of existing geographic units such as states and then summarize both a statistical and geographic distrubution by linking statistical summaries to a series of small maps
- In this case we're looking at summaries of both pverty and education by US states
</div>
```{r, echo=FALSE, warning=FALSE, message=FALSE, eval=FALSE}
library(micromap)
data("edPov")
data("USstates")
statePolys <- create_map_table(USstates, 'ST')
mmplot(stat.data=edPov,  map.data=statePolys ,
       panel.types=c('labels', 'dot', 'dot','map'),
       panel.data=list('state','pov','ed', NA),
       ord.by='pov',  
       grouping=5, 
       median.row=T,
       map.link=c('StateAb','ID'),
       
       plot.height=9,  						
       colors=c('red','orange','green','blue','purple'), 
       map.color2='lightgray',
       
       
       panel.att=list(list(1, header='States', panel.width=.8, align='left', text.size=.9),
                      list(2, header='Percent Living Below \n Poverty Level',
                           graph.bgcolor='lightgray', point.size=1.5,
                           xaxis.ticks=list(10,15,20), xaxis.labels=list(10,15,20),
                           xaxis.title='Percent'),
                      list(3, header='Percent Adults With\n4+ Years of College',
                           graph.bgcolor='lightgray', point.size=1.5,
                           xaxis.ticks=list(0,20,30,40), xaxis.labels=list(0,20,30,40),
                           xaxis.title='Percent'),
                      list(4, header='Light Gray Means\nHighlighted Above',  
                           inactive.border.color=gray(.7), inactive.border.size=2,	
                           panel.width=.8)))
```
<div class="centered">
<img src="Micromap.jpeg" alt="Micromap" style="width: 400px;"/>
</div>
## Some things still better in python { .smaller}
### R Markdown + knitr package
<div class="notes">
- There are times when certain tasks with certain data are better done using python or another programming language
- R Markdown is a format that allows easy creation of dynamic documents, presentations, and reports from R. 
- Combines markdown syntax with embedded R code chunks that are run so their output can be included in the final document. 
- R Markdown documents are fully reproducible (they can be automatically regenerated whenever underlying R code or data changes).
- knitr package, used with RStudio, basically extends and makes easy creating dynamic documents using R markdown
- Great thing with knitr is that you can actually fold in programming languages other than R into your R markdown
- here I've got a code that will run a python chunk of code, and then run an R chunk of code
- I combine all my R and python work into one R document this way, that I can then generate an html, Word, or pdf document or slide show and make work reproducible
</div>
<div class="centered">
<img src="KnitrRPython.png" alt="Roadmap" style="width: 600px;"/>
</div>

## Combining R and Python { .smaller}
### R Markdown + knitr package
<div class="notes">
- Here we can see output generated in my Markdown document when I 'knit' the R Markdown in the previous document.  
- Show in RStudio
</div>
<div class="centered">
<img src="levelplotElev.png" alt="Roadmap" style="width: 650px;"/>
</div>
## Resources
- https://github.com/Robinlovelace/Creating-maps-in-R
- https://github.com/Pakillo/R-GIS-tutorial/blob/master/R-GIS_tutorial.md
- http://www.maths.lancs.ac.uk/~rowlings/Teaching/Sheffield2013/spatialops.html
- http://www.maths.lancs.ac.uk/~rowlings/Teaching/UseR2012/cheatsheet.html
- https://science.nature.nps.gov/im/datamgmt/statistics/r/advanced/spatial.cfm
- http://www.asdar-book.org/ 
- http://pakillo.github.io/R-GIS-tutorial/
